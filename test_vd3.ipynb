{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "455c4e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So sánh thuật toán GDA vs GD\n",
      "============================================================\n",
      "\n",
      "Chạy cho n = 2:\n",
      "KKT violation = False, grad norm = 20.388686, product x_i = 1.000000\n",
      "GDA: f(x*) = 22.157584, #Iter =   31, Time = 0.2679s\n",
      "KKT violation = False, grad norm = 20.388686, product x_i = 1.000000\n",
      "GD : f(x*) = 22.157584, #Iter =   18, Time = 0.0988s\n",
      "\n",
      "================================================================================\n",
      "KẾT QUẢ CHO n = 2\n",
      "================================================================================\n",
      "| n   | GDA: f(x*)  | #Iter | Time (s)  | GD: f(x*)   | #Iter | Time (s)  |\n",
      "|-----|-------------|-------|-----------|-------------|-------|-----------|\n",
      "| 2   |   22.157584 |    31 |    0.2679 |   22.157584 |    18 |    0.0988 |\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import autograd.numpy as anp\n",
    "import time\n",
    "from autograd import grad\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def objective_function(x, a, alpha, beta, e):\n",
    "    try:\n",
    "        if np.any(np.isnan(x)) or np.any(np.isinf(x)):\n",
    "            raise ValueError(\"Input x contains NaN or Inf\")\n",
    "        \n",
    "        term1 = anp.dot(a, x)\n",
    "        term2 = alpha * anp.dot(x, x)\n",
    "        xtx = anp.clip(anp.dot(x, x), 0, 1e10)\n",
    "        term3 = beta / anp.sqrt(1 + beta * xtx) * anp.dot(e, x)\n",
    "        result = term1 + term2 + term3\n",
    "        \n",
    "        if np.isnan(result) or np.isinf(result):\n",
    "            raise ValueError(\"Objective function returned NaN or Inf\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error in objective_function: {e}\")\n",
    "        raise\n",
    "\n",
    "def gradient_f(x, a, alpha, beta, e, n):\n",
    "    try:\n",
    "        if np.any(np.isnan(x)) or np.any(np.isinf(x)):\n",
    "            raise ValueError(\"Input x contains NaN or Inf\")\n",
    "        \n",
    "        term1 = a\n",
    "        term2 = 2 * alpha * x\n",
    "        xtx = anp.clip(anp.dot(x, x), 0, 1e10)\n",
    "        sqrt_term = anp.sqrt(1 + beta * xtx)\n",
    "        term3 = (beta / sqrt_term) * e - (beta**2 * anp.dot(e, x) / (sqrt_term**3)) * x\n",
    "        grad = term1 + term2 + term3\n",
    "        \n",
    "        if np.any(np.isnan(grad)) or np.any(np.isinf(grad)):\n",
    "            raise ValueError(\"Gradient returned NaN or Inf\")\n",
    "        return grad\n",
    "    except Exception as e:\n",
    "        print(f\"Error in gradient_f: {e}\")\n",
    "        raise\n",
    "\n",
    "def project_C(z, n, tol=1e-6):\n",
    "    try:\n",
    "        if np.any(np.isnan(z)) or np.any(np.isinf(z)):\n",
    "            raise ValueError(\"Input z contains NaN or Inf\")\n",
    "        \n",
    "        z = np.maximum(np.minimum(z, 1e6), tol)\n",
    "        \n",
    "        def obj(x):\n",
    "            return 0.5 * anp.sum((x - z)**2)\n",
    "        def constraint(x):\n",
    "            return anp.sum(anp.log(x))\n",
    "        constraints = {'type': 'ineq', 'fun': constraint}\n",
    "        bounds = [(tol, None)] * n\n",
    "        \n",
    "        x_init = np.maximum(z, tol)\n",
    "        if anp.sum(anp.log(x_init)) < 0 or np.any(x_init <= 0):\n",
    "            geo_mean = np.prod(z) ** (1/n)\n",
    "            x_init = np.ones(n) * max(tol, geo_mean * np.exp(0.1))\n",
    "        \n",
    "        result = minimize(obj, x0=x_init, constraints=constraints, bounds=bounds, method='SLSQP',\n",
    "                         options={'ftol': 1e-8, 'maxiter': 2000, 'disp': False})\n",
    "        if not result.success:\n",
    "            print(f\"Projection failed: {result.message}, z={z[:5]}...\")\n",
    "            x_init = np.ones(n) * max(tol, np.prod(z) ** (1/n) * np.exp(0.2))\n",
    "            result = minimize(obj, x0=x_init, constraints=constraints, bounds=bounds, method='SLSQP',\n",
    "                            options={'ftol': 1e-8, 'maxiter': 2000, 'disp': False})\n",
    "            if not result.success:\n",
    "                raise RuntimeError(f\"Projection fallback failed: {result.message}\")\n",
    "        \n",
    "        return result.x\n",
    "    except Exception as e:\n",
    "        print(f\"Error in project_C: {e}\")\n",
    "        raise\n",
    "\n",
    "def gda_adaptive_algorithm(n, max_iter=10000, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    beta = 0.741271\n",
    "    alpha = 3 * (beta ** (3/2)) * np.sqrt(n) + 1\n",
    "    L = 4 * (beta ** (3/2)) * np.sqrt(n) + 3 * alpha\n",
    "\n",
    "    a = np.random.uniform(1, 10, n)\n",
    "    e = np.arange(1, n + 1, dtype=float)\n",
    "    \n",
    "    x_k = np.random.uniform(0.5, 1.5, n)\n",
    "    x_k = project_C(x_k, n)\n",
    "    \n",
    "    lambda_k = 1/L  # Giảm bước ban đầu\n",
    "    sigma = 0.1\n",
    "    kappa = 0.9\n",
    "    max_armijo_iter = 20\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        grad_k = gradient_f(x_k, a, alpha, beta, e, n)\n",
    "        grad_norm = np.linalg.norm(grad_k)\n",
    "        \n",
    "        if grad_norm < 1e-3:  # Thêm điều kiện dừng dựa trên gradient norm\n",
    "            print(f\"Stopped due to small gradient norm: {grad_norm:.6f}\")\n",
    "            break\n",
    "            \n",
    "        f_current = objective_function(x_k, a, alpha, beta, e)\n",
    "        lambda_current = lambda_k\n",
    "        armijo_iter = 0\n",
    "        \n",
    "        while armijo_iter < max_armijo_iter:\n",
    "            step = np.minimum(lambda_current * np.linalg.norm(grad_k), 1.0) * grad_k / np.linalg.norm(grad_k) if np.linalg.norm(grad_k) > 0 else grad_k\n",
    "            x_candidate = x_k - step\n",
    "            x_next = project_C(x_candidate, n)\n",
    "            f_next = objective_function(x_next, a, alpha, beta, e)\n",
    "            armijo_rhs = f_current - sigma * np.dot(grad_k, x_k - x_next)\n",
    "            \n",
    "            if f_next <= armijo_rhs:\n",
    "                break\n",
    "            else:\n",
    "                lambda_current *= kappa\n",
    "                armijo_iter += 1\n",
    "        \n",
    "        if armijo_iter >= max_armijo_iter:\n",
    "            step = np.minimum(lambda_current * np.linalg.norm(grad_k), 1.0) * grad_k / np.linalg.norm(grad_k) if np.linalg.norm(grad_k) > 0 else grad_k\n",
    "            x_next = project_C(x_k - step, n)\n",
    "\n",
    "        if lambda_current < 1e-10:\n",
    "            print(f\"Step size too small at iteration {k}\")\n",
    "            break\n",
    "\n",
    "        if np.linalg.norm(x_next - x_k) < 1e-8:\n",
    "            break\n",
    "\n",
    "        x_k = x_next\n",
    "        lambda_k = lambda_current\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    f_final = objective_function(x_k, a, alpha, beta, e)\n",
    "    \n",
    "    grad_final = gradient_f(x_k, a, alpha, beta, e, n)\n",
    "    grad_norm = np.linalg.norm(grad_final)\n",
    "    kkt_violation = np.any(grad_final[x_k <= 1e-6] < -1e-6)\n",
    "    print(f\"KKT violation = {kkt_violation}, grad norm = {grad_norm:.6f}, product x_i = {np.prod(x_k):.6f}\")\n",
    "    \n",
    "    return x_k, f_final, k + 1, elapsed_time, a, e\n",
    "\n",
    "def gd_algorithm(n, max_iter=10000, seed=0, a=None, e=None):\n",
    "    np.random.seed(seed)\n",
    "    beta = 0.741271\n",
    "    alpha = 3 * (beta ** (3/2)) * np.sqrt(n) + 1\n",
    "    L = 4 * (beta ** (3/2)) * np.sqrt(n) + 3 * alpha\n",
    "    lambda_k = 1 / L\n",
    "\n",
    "    if a is None:\n",
    "        a = np.random.uniform(1, 10, n)\n",
    "    if e is None:\n",
    "        e = np.arange(1, n + 1, dtype=float)\n",
    "        \n",
    "    x_k = np.random.uniform(0.5, 1.5, n)\n",
    "    x_k = project_C(x_k, n)\n",
    "\n",
    "    start_time = time.time()\n",
    "    for k in range(max_iter):\n",
    "        grad = gradient_f(x_k, a, alpha, beta, e, n)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        \n",
    "        if grad_norm < 1e-3:\n",
    "            print(f\"Stopped due to small gradient norm: {grad_norm:.6f}\")\n",
    "            break\n",
    "            \n",
    "        step = np.minimum(lambda_k * np.linalg.norm(grad), 1.0) * grad / np.linalg.norm(grad) if np.linalg.norm(grad) > 0 else grad\n",
    "        x_next = project_C(x_k - step, n)\n",
    "        \n",
    "        if np.linalg.norm(x_next - x_k) < 1e-8:\n",
    "            break\n",
    "        x_k = x_next\n",
    "        \n",
    "    elapsed_time = time.time() - start_time\n",
    "    f_final = objective_function(x_k, a, alpha, beta, e)\n",
    "    \n",
    "    grad_final = gradient_f(x_k, a, alpha, beta, e, n)\n",
    "    grad_norm = np.linalg.norm(grad_final)\n",
    "    kkt_violation = np.any(grad_final[x_k <= 1e-6] < -1e-6)\n",
    "    print(f\"KKT violation = {kkt_violation}, grad norm = {grad_norm:.6f}, product x_i = {np.prod(x_k):.6f}\")\n",
    "    \n",
    "    return x_k, f_final, k + 1, elapsed_time\n",
    "\n",
    "def run_comparison(n):\n",
    "    print(\"So sánh thuật toán GDA vs GD\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(f\"\\nChạy cho n = {n}:\")\n",
    "    \n",
    "    x_opt_gda, f_opt_gda, iters_gda, time_gda, a, e = gda_adaptive_algorithm(n)\n",
    "    print(f\"GDA: f(x*) = {f_opt_gda:.6f}, #Iter = {iters_gda:4d}, Time = {time_gda:.4f}s\")\n",
    "    \n",
    "    x_opt_gd, f_opt_gd, iters_gd, time_gd = gd_algorithm(n, a=a, e=e)\n",
    "    print(f\"GD : f(x*) = {f_opt_gd:.6f}, #Iter = {iters_gd:4d}, Time = {time_gd:.4f}s\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"KẾT QUẢ CHO n = {n}\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"| n   | GDA: f(x*)  | #Iter | Time (s)  | GD: f(x*)   | #Iter | Time (s)  |\")\n",
    "    print(\"|-----|-------------|-------|-----------|-------------|-------|-----------|\")\n",
    "    print(f\"| {n:<3} | {f_opt_gda:>11.6f} | {iters_gda:>5} | {time_gda:>9.4f} | {f_opt_gd:>11.6f} | {iters_gd:>5} | {time_gd:>9.4f} |\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n = 2\n",
    "    run_comparison(n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
