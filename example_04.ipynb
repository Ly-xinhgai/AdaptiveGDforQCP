{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49d4a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from autograd import grad\n",
    "import autograd.numpy as np1\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import time\n",
    "from scipy.optimize import BFGS, SR1\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.optimize import LinearConstraint\n",
    "from scipy.optimize import NonlinearConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb4404ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result GDA - x*: [0.67731163 0.95949163 0.35258189 0.82605123 0.43245048 0.54599655\n",
      " 1.04067074 1.14944462 1.01656582 0.49802665]\n",
      "Value -ln(-f(x*)) of GDA: 6.356830638722489\n",
      "Result RNN - x*: [[0.23566054 1.03431672 0.60411088 1.03941566 0.48740331 0.47697122\n",
      "  0.63954983 1.03950417 0.88332332 1.2327445 ]]\n",
      "Value -ln(-f(x*)) of RNN: 6.825257359310671\n"
     ]
    }
   ],
   "source": [
    "# from IPython import display\n",
    "# display.Image(\"8143e0da24f78ea9d7e6.jpg\")\n",
    "import numpy as np\n",
    "from autograd import grad\n",
    "import autograd.numpy as np1\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import random\n",
    "import time\n",
    "from scipy.optimize import BFGS,SR1\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.optimize import LinearConstraint\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "\n",
    "# RK4 method\n",
    "def ode_solve_G(z0, G):\n",
    "    \"\"\"\n",
    "    Simplest RK4 ODE initial value solver\n",
    "    \"\"\"\n",
    "    n_steps = 500\n",
    "    z = z0\n",
    "    # print(z)\n",
    "    h = np.array([0.05])\n",
    "    for i_step in range(n_steps):\n",
    "        k1 = h*G(z)\n",
    "\n",
    "        k2 = h * (G((z+h/2)))\n",
    "        k3 = h * (G((z+h/2)))\n",
    "        k4 = h * (G((z+h)))\n",
    "        k = (1/6)*(k1+2*k2+2*k3+k4)\n",
    "        #k = k.reshape(1,10)\n",
    "        z = np.array([z0]).reshape(10,1)\n",
    "        z = z + k\n",
    "        #print(\"z;\",z.shape)\n",
    "    return z\n",
    "# def f(x):\n",
    "#     return (x[0]**2 + x[1]**2 + 3) / (1 + 2*x[0] + 8*x[1])\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Objective function f(x) as defined in the problem.\n",
    "    x: vector of variables\n",
    "    q: vector of parameters\n",
    "    \"\"\"\n",
    "    # tmp = []\n",
    "    # for i in range(10):\n",
    "    #     tmp.append(np.random.rand(1).tolist()[0])\n",
    "    tmp = 10*[1]\n",
    "    # print(tmp)\n",
    "    q = np1.array(tmp)\n",
    "    return -np1.exp(-np1.sum((x**2) / (q**2)))\n",
    "# def g1(x):\n",
    "#     return -x[0]**2 - 2*x[0]*x[1] + 4\n",
    "# def g2(x):\n",
    "#     return -x[0]\n",
    "# def g3(x):\n",
    "#     return -x[1]\n",
    "\n",
    "def g_i(x):\n",
    "    i =1\n",
    "    \"\"\"\n",
    "    Inequality constraint g_i(x) as defined in the problem with a sequence of squared terms.\n",
    "    x: vector of variables\n",
    "    i: the index of the inequality constraint function g_i\n",
    "    \"\"\"\n",
    "    # Compute the sequence of squared terms\n",
    "    # We adjust the indices for 0-based indexing used in Python.\n",
    "    # The sequence is x_{10*(i-1)+1}^2 to x_{10*(i-1)+10}^2, hence we use a slice [10*(i-1):10*i]\n",
    "    squared_terms = x[10*(i-1):10*i]**2\n",
    "\n",
    "    # Compute the inequality constraint function value\n",
    "    g_x_i = np1.sum(squared_terms) - 20\n",
    "    \n",
    "    return g_x_i\n",
    "def derivative_g_i(x):\n",
    "    i = 1\n",
    "    \"\"\"\n",
    "    Derivative of the inequality constraint g_i(x) with respect to x, as defined in the problem.\n",
    "    x: vector of variables\n",
    "    i: the index of the inequality constraint function g_i\n",
    "    \"\"\"\n",
    "    # Initialize the gradient as a zero vector of the same length as x\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    # Compute the gradient only for the terms involved in the ith inequality\n",
    "    # Adjust for 0-based indexing: the terms are x_{10*(i-1)+1} to x_{10*(i-1)+10}\n",
    "    indices = range(10*(i-1), 10*i)\n",
    "    grad[indices] = 2 * x[indices]\n",
    "\n",
    "    return grad\n",
    "def g3(x):\n",
    "    x = np.array(x)\n",
    "    A = np.array([[1,1,1,1,1,3,3,3,3,3]])\n",
    "    b = np.array([[16]])\n",
    "    return (A@(x.T) - b.T).tolist()[0][0] # \n",
    "# g1_dx = grad(g1)\n",
    "# g2_dx = grad(g2)\n",
    "# g3_dx = grad(g3)\n",
    "# g_dx = [g1_dx,g2_dx]\n",
    "f_dx = grad(f)\n",
    "# bounds = Bounds([0,0],[np.inf,np.inf])\n",
    "cons = ({'type': 'eq',\n",
    "          'fun' : lambda x: np.array([g3(x)]),\n",
    "          'jac' : lambda x: np.array([1,1,1,1,1,3,3,3,3,3])},\n",
    "        {'type': 'ineq',\n",
    "          'fun' : lambda x: np.array([-g_i(x)]),\n",
    "          'jac' : lambda x: np.array([-grad(g_i)(x)])})\n",
    "def rosen(x,y):\n",
    "    \"\"\"The Rosenbrock function\"\"\"\n",
    "    return np.sqrt(np.sum((x-y)**2))\n",
    "def find_min(y,n):\n",
    "    x = np.random.rand(1,n).tolist()[0]\n",
    "    res = minimize(rosen, x, args=(y), jac=\"2-point\",hess=BFGS(),\n",
    "                constraints=cons,method='trust-constr', options={'disp': False})\n",
    "    return res.x\n",
    "def run_nonsmooth1(x, max_iters, f, f_dx,n,alpha,mu0):\n",
    "    res = []\n",
    "    val = []\n",
    "    lda = 1 #1e9\n",
    "    sigma = 0.1 #100\n",
    "    mut = mu0\n",
    "    K = np.random.rand(1,1)\n",
    "    res.append(x)\n",
    "    val.append(f(x))\n",
    "    x_pre = x\n",
    "    for t in range(max_iters):\n",
    "        y = x - lda*f_dx(x)\n",
    "        x_pre = x.copy()\n",
    "        x = find_min(y,n)\n",
    "        if f(x) - f(x_pre) + sigma*(np.dot(f_dx(x_pre).T,x_pre - x)) <= 0:\n",
    "            lda = lda\n",
    "        else:\n",
    "            lda = K*lda\n",
    "        #mut = mut*np.exp(-alpha*t)\n",
    "        res.append(x)\n",
    "        val.append(f(x))\n",
    "    #print(x)\n",
    "    return res,x\n",
    "def Phi(s):\n",
    "    if s > 0:\n",
    "        return 1\n",
    "    elif s == 0:\n",
    "        return np.random.rand(1)\n",
    "    return 0\n",
    "# Neural network\n",
    "A = np.array([[1,1,1,1,1,3,3,3,3,3]])\n",
    "b = np.array([[16]])\n",
    "def G(x):\n",
    "\n",
    "   \n",
    "    #g3x = g3(x)\n",
    "    gx = [g_i(x)]\n",
    "    g_dx = [grad(g_i)]\n",
    "    c_xt = 1.\n",
    "    Px = np.zeros((10, 1))\n",
    "\n",
    "    for (i,j) in zip(gx, g_dx):\n",
    "        c_xt *= (1-Phi(i))\n",
    "        #print(Phi(i)*(j(x)))\n",
    "        # print(j)\n",
    "        # print(Phi(i))\n",
    "        # print(Px)\n",
    "        #print(Phi(i),j(x))\n",
    "        # print(\"Px:\",Px)\n",
    "        # print(x)\n",
    "        # print(np.array([Phi(i)*j(x)]))\n",
    "        #print(x.shape)\n",
    "        Px += np.array([Phi(i)*j(x)]).reshape(10,1)\n",
    "    c_xt *= (1-Phi(np.abs(A@(x) - b)))\n",
    "    \n",
    "    eq_constr_dx = ((2*Phi(A@(x)-b)-1)*A.T)\n",
    "    # print(-c_xt*f_dx(x))\n",
    "    # print(Px)\n",
    "    # print(eq_constr_dx)\n",
    "    #print((np.array([-c_xt*f_dx(x)]).reshape(10,1) - Px - eq_constr_dx) .shape)\n",
    "    #print(((2*Phi(A@(x)-b)-1)*A.T).shape)\n",
    "    return np.array([-c_xt*f_dx(x)]).reshape(10,1) - Px - eq_constr_dx\n",
    "def run_nonsmooth(x0, max_iters):\n",
    "    xt = x0\n",
    "    res = []\n",
    "    res.append(xt.tolist())\n",
    "    for t in range(max_iters):\n",
    "        xt = ode_solve_G(xt,G)\n",
    "        #print(xt.shape)\n",
    "        #print(xt.shape)\n",
    "        #print(xt.reshape(1,10).tolist())\n",
    "        res.append(xt.reshape(1,10).tolist()[0])\n",
    "    # print(xt)\n",
    "    # print(f(xt))\n",
    "    return res,xt.reshape(1,10)\n",
    "\n",
    "def plot_x(sol_all,count,max_iters):\n",
    "    t = [i for i in range(max_iters+1)]\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    for i in range(count):\n",
    "        if i ==0:\n",
    "            text_color = 'red'\n",
    "            text_label = r'$x_{1}(t)$'\n",
    "        else:\n",
    "            text_color = 'green'\n",
    "            text_label = r'$x_{2}(t)$'\n",
    "        plt.plot(t, sol_all[i][:,0],color=text_color,label=text_label,linewidth=1)\n",
    "        plt.plot(t, sol_all[i][:,1],color=text_color,linewidth=1)\n",
    "        plt.plot(t, sol_all[i][:,2],color=text_color,linewidth=1)\n",
    "        plt.plot(t, sol_all[i][:,3],color=text_color,linewidth=1)\n",
    "        plt.plot(t, sol_all[i][:,4],color=text_color,linewidth=1)\n",
    "        plt.plot(t, sol_all[i][:,5],color=text_color,linewidth=1)\n",
    "        plt.plot(t, sol_all[i][:,6],color=text_color,linewidth=1)\n",
    "        plt.plot(t, sol_all[i][:,7],color=text_color,linewidth=1)\n",
    "        plt.plot(t, sol_all[i][:,8],color=text_color,linewidth=1)\n",
    "        plt.plot(t, sol_all[i][:,9],color=text_color,linewidth=1)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('x(t)')\n",
    "    plt.legend([r'$x_{1}(t)$',r'$x_{2}(t)$']) #,r'$x_{4}(t)$',r'$x_{5}(t)$',r'$x_{6}(t)$',r'$x_{7}(t)$',r'$x_{8}(t)$',r'$x_{9}(t)$',r'$x_{10}(t)$'])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def main_GDA(num, max_iters, n, alpha, mu0):\n",
    "    sol_all1 = []\n",
    "    for i in range(num):\n",
    "        x0 = np.random.rand(1, n)\n",
    "        x0 = find_min(x0, n)  # Start point\n",
    "        _, xt = run_nonsmooth1(x0, max_iters, f, f_dx, n, alpha, mu0)\n",
    "        print(\"Result GDA - x*:\", xt)\n",
    "        print(\"Value -ln(-f(x*)) of GDA:\", -np.log(-f(xt)))\n",
    "        sol_all1.append(xt)\n",
    "    return sol_all1\n",
    "\n",
    "def main_RNN(num, max_iters, n):\n",
    "    sol_all = []\n",
    "    for i in range(num):\n",
    "        x0 = np.random.rand(1, n)\n",
    "        x0 = find_min(x0, n)  # Start point\n",
    "        _, xt = run_nonsmooth(x0, max_iters)\n",
    "        print(\"Result RNN - x*:\", xt)\n",
    "        print(\"Value -ln(-f(x*)) of RNN:\", -np.log(-f(xt)))\n",
    "        sol_all.append(xt)\n",
    "    return sol_all\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Main parameters\n",
    "    num = 1  # Number of starting points\n",
    "    max_iters = 10  # Maximum number of iterations\n",
    "    n = 10  # Size of x\n",
    "    alpha = np.random.rand(1)  # Alpha parameter\n",
    "    mu0 = np.random.rand(1)  # Mu0 parameter\n",
    "\n",
    "    # Run the main function for GDA\n",
    "    result_GDA = main_GDA(num, max_iters, n, alpha, mu0)\n",
    "\n",
    "    # Run the main function for RNN\n",
    "    result_RNN = main_RNN(num, max_iters, n)\n",
    "\n",
    "    # Plot trajectory\n",
    "    # plot_x(sol_all1,count,max_iters1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
